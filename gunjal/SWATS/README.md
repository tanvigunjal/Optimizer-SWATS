# SWATS-Optimizer: Switching from Adam to SGD


# Overview

# Experiment

# Setup

# Conclusion

# References 

<a id="1">[1]</a> 
Improving Generalization Performance by Switching from Adam to SGD.<a href="
https://doi.org/10.48550/arXiv.1712.07628
">Link</a>

<a id="2">[2]</a> 
Adam: A Method for Stochastic Optimization.<a href="
https://doi.org/10.48550/arXiv.1412.6980
">Link</a>

<a id="3">[3]</a> 
An overview of gradient descent optimization
algorithms.<a href="
https://arxiv.org/pdf/1609.04747.pdf
">Link</a>